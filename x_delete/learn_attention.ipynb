{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input embeddings: tensor([[[0.3737, 0.9901, 0.2007, 0.5810],\n",
      "         [0.5722, 0.1240, 0.1568, 0.2650],\n",
      "         [0.2675, 0.7471, 0.7178, 0.2514]],\n",
      "\n",
      "        [[0.2130, 0.4490, 0.0515, 0.8130],\n",
      "         [0.6847, 0.5774, 0.8823, 0.9048],\n",
      "         [0.3371, 0.3823, 0.2571, 0.8125]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Example input: batch of sentences with word embeddings\n",
    "# Assume we have a batch of 2 sentences, each with 3 words, and an embedding size of 4\n",
    "batch_size = 2\n",
    "seq_length = 3\n",
    "embedding_dim = 4\n",
    "\n",
    "# Random input embeddings (batch_size, seq_length, embedding_dim)\n",
    "input_embeddings = torch.rand(batch_size, seq_length, embedding_dim)\n",
    "\n",
    "print(\"Input embeddings:\", input_embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Define Linear Layers for Q, K, V Transformations\n",
    "We need to define the linear transformations for Q, K, and V:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_k = 4  # Dimension of keys/queries\n",
    "d_v = 4  # Dimension of values\n",
    "\n",
    "# Linear layers for transforming input embeddings to Q, K, V\n",
    "W_q = nn.Linear(embedding_dim, d_k)\n",
    "W_k = nn.Linear(embedding_dim, d_k)\n",
    "W_v = nn.Linear(embedding_dim, d_v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the output of a linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights:\n",
      "Parameter containing:\n",
      "tensor([[-0.3365, -0.1809, -0.0087, -0.4713],\n",
      "        [-0.0403, -0.0960, -0.2113,  0.1327],\n",
      "        [-0.2775,  0.3236, -0.0979,  0.2060],\n",
      "        [-0.3081,  0.3066,  0.3339, -0.2492]], requires_grad=True)\n",
      "\n",
      "Biases:\n",
      "Parameter containing:\n",
      "tensor([-0.0809,  0.2758, -0.2320,  0.2945], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(\"Weights:\")\n",
    "print(W_q.weight)\n",
    "\n",
    "print(\"\\nBiases:\")\n",
    "print(W_q.bias)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Apply Linear Transformations to Get Q, K, V\n",
    "We transform the input embeddings into Q, K, and V:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4])\n",
      "Q: tensor([[[-0.6613,  0.2004,  0.0847,  0.4051],\n",
      "         [-0.4221,  0.2429, -0.3114,  0.1426],\n",
      "         [-0.4308,  0.0750, -0.0830,  0.6181]],\n",
      "\n",
      "        [[-0.6174,  0.3211,  0.0166,  0.1811],\n",
      "         [-0.8498,  0.1264, -0.1352,  0.3296],\n",
      "         [-0.6486,  0.2790, -0.0596,  0.1912]]], grad_fn=<ViewBackward0>)\n",
      "K: tensor([[[-0.2621, -0.4322,  0.4397, -0.1514],\n",
      "         [-0.6064,  0.0798,  0.3596,  0.1834],\n",
      "         [-0.5191,  0.0446,  0.6768, -0.4237]],\n",
      "\n",
      "        [[-0.3610, -0.4171,  0.1336,  0.0934],\n",
      "         [-0.9146, -0.0535,  0.6703, -0.0579],\n",
      "         [-0.5351, -0.2750,  0.2517,  0.0883]]], grad_fn=<ViewBackward0>)\n",
      "V: tensor([[[0.5564, 0.6698, 0.3383, 0.8247],\n",
      "         [0.1886, 0.2945, 0.1173, 0.2909],\n",
      "         [0.6719, 0.6491, 0.6703, 0.8188]],\n",
      "\n",
      "        [[0.2928, 0.4051, 0.0033, 0.6127],\n",
      "         [0.4489, 1.0383, 0.3072, 1.1090],\n",
      "         [0.2925, 0.5191, 0.0530, 0.6824]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Transform the input embeddings\n",
    "print(input_embeddings.shape)\n",
    "Q = W_q(input_embeddings)  # (batch_size, seq_length, d_k)\n",
    "K = W_k(input_embeddings)  # (batch_size, seq_length, d_k)\n",
    "V = W_v(input_embeddings)  # (batch_size, seq_length, d_v)\n",
    "\n",
    "print(\"Q:\", Q)\n",
    "print(\"K:\", K)\n",
    "print(\"V:\", V)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 5, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "A1 = torch.rand(4,5,3,4)\n",
    "A2 = torch.rand(4,2)\n",
    "\n",
    "A3 = A1@A2\n",
    "\n",
    "print(A3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code example from this page:\n",
    "https://www.analyticsvidhya.com/blog/2024/04/understanding-transformers-a-deep-dive-into-nlps-core-technology/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output after self-attention:\n",
      "tensor([[ 0.2783, -1.5008,  0.6212],\n",
      "        [ 0.2857, -1.5538,  0.6385],\n",
      "        [ 0.2930, -1.6056,  0.6554]])\n"
     ]
    }
   ],
   "source": [
    "#import libraries\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Example input sequence\n",
    "input_sequence = torch.tensor([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9]])\n",
    "\n",
    "# Generate random weights for Key, Query, and Value matrices\n",
    "random_weights_key = torch.randn(input_sequence.size(-1), input_sequence.size(-1))\n",
    "random_weights_query = torch.randn(input_sequence.size(-1), input_sequence.size(-1))\n",
    "random_weights_value = torch.randn(input_sequence.size(-1), input_sequence.size(-1))\n",
    "\n",
    "# Compute Key, Query, and Value matrices\n",
    "key = torch.matmul(input_sequence, random_weights_key)\n",
    "query = torch.matmul(input_sequence, random_weights_query)\n",
    "value = torch.matmul(input_sequence, random_weights_value)\n",
    "\n",
    "# Compute attention scores\n",
    "attention_scores = torch.matmul(query, key.T) / torch.sqrt(torch.tensor(query.size(-1),\n",
    "dtype=torch.float32))\n",
    "\n",
    "# Apply softmax to obtain attention weights\n",
    "attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "\n",
    "# Compute weighted sum of Value vectors\n",
    "output = torch.matmul(attention_weights, value)\n",
    "\n",
    "print(\"Output after self-attention:\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Size of Q, K, and V Matrices\n",
    "\n",
    "1. **Input Sequence:**\n",
    "   - Number of words (sequence length): \\( N = 10 \\)\n",
    "   - Embedding dimension: \\( d_{\\text{model}} = 512 \\)\n",
    "\n",
    "2. **Weights for Q, K, and V:**\n",
    "   - The weight matrices \\( W_Q \\), \\( W_K \\), and \\( W_V \\) are used to project the input embeddings into the query, key, and value spaces.\n",
    "\n",
    "3. **Dimensionality of Q, K, and V:**\n",
    "   - The dimensions of the weight matrices are typically \\( d_{\\text{model}} \\times d_k \\) for \\( W_Q \\) and \\( W_K \\), and \\( d_{\\text{model}} \\times d_v \\) for \\( W_V \\), where \\( d_k \\) and \\( d_v \\) are often equal to \\( d_{\\text{model}} \\) for simplicity, but they can be different depending on the implementation.\n",
    "\n",
    "4. **Projection Matrices:**\n",
    "   - \\( W_Q \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k} \\)\n",
    "   - \\( W_K \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k} \\)\n",
    "   - \\( W_V \\in \\mathbb{R}^{d_{\\text{model}} \\times d_v} \\)\n",
    "\n",
    "For simplicity, let's assume \\( d_k = d_v = d_{\\text{model}} \\):\n",
    "\n",
    "- \\( W_Q \\in \\mathbb{R}^{512 \\times 512} \\)\n",
    "- \\( W_K \\in \\mathbb{R}^{512 \\times 512} \\)\n",
    "- \\( W_V \\in \\mathbb{R}^{512 \\times 512} \\)\n",
    "\n",
    "### Computation of Q, K, and V:\n",
    "\n",
    "Given an input sequence \\( X \\) of shape \\( (N, d_{\\text{model}}) = (10, 512) \\):\n",
    "\n",
    "- **Query Matrix (Q):**\n",
    "  \\[ Q = X \\cdot W_Q \\]\n",
    "  - Shape of \\( Q \\): \\( (10, 512) \\)\n",
    "\n",
    "- **Key Matrix (K):**\n",
    "  \\[ K = X \\cdot W_K \\]\n",
    "  - Shape of \\( K \\): \\( (10, 512) \\)\n",
    "\n",
    "- **Value Matrix (V):**\n",
    "  \\[ V = X \\cdot W_V \\]\n",
    "  - Shape of \\( V \\): \\( (10, 512) \\)\n",
    "\n",
    "### Attention Mechanism:\n",
    "\n",
    "The attention scores are computed as:\n",
    "\\[ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V \\]\n",
    "\n",
    "### Example Calculation:\n",
    "\n",
    "1. **Input Sequence:**\n",
    "   - \\( X \\) has a shape of \\( (10, 512) \\).\n",
    "\n",
    "2. **Weight Matrices:**\n",
    "   - \\( W_Q \\), \\( W_K \\), and \\( W_V \\) each have a shape of \\( (512, 512) \\).\n",
    "\n",
    "3. **Query, Key, and Value Projections:**\n",
    "   - \\( Q = X \\cdot W_Q \\): Shape \\( (10, 512) \\)\n",
    "   - \\( K = X \\cdot W_K \\): Shape \\( (10, 512) \\)\n",
    "   - \\( V = X \\cdot W_V \\): Shape \\( (10, 512) \\)\n",
    "\n",
    "### Summary\n",
    "\n",
    "In this scenario, with an input sequence of 10 words and 512-dimensional embeddings:\n",
    "- The query, key, and value matrices \\( Q \\), \\( K \\), and \\( V \\) will each have a shape of \\( (10, 512) \\).\n",
    "- The weight matrices \\( W_Q \\), \\( W_K \\), and \\( W_V \\) will each have a shape of \\( (512, 512) \\).\n",
    "\n",
    "These dimensions ensure that the attention mechanism can compute attention scores and attended values effectively, capturing the relationships and dependencies between tokens in the sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatGPT Generated Code\n",
    "import torch\n",
    "\n",
    "# Example input sequence\n",
    "input_sequence = torch.randn(10, 512)  # 10 words, 512-dimensional embeddings\n",
    "\n",
    "# Generate random weights for Key, Query, and Value matrices\n",
    "random_weights_key = torch.randn(input_sequence.size(-1), input_sequence.size(-1))\n",
    "random_weights_query = torch.randn(input_sequence.size(-1), input_sequence.size(-1))\n",
    "random_weights_value = torch.randn(input_sequence.size(-1), input_sequence.size(-1))\n",
    "\n",
    "print(\"Size of input_sequence:\", input_sequence.size())\n",
    "print(\"Size of random_weights_key:\", random_weights_key.size())\n",
    "print(\"Size of random_weights_query:\", random_weights_query.size())\n",
    "print(\"Size of random_weights_value:\", random_weights_value.size())\n",
    "\n",
    "# Compute Q, K, and V matrices\n",
    "Q = input_sequence @ random_weights_query\n",
    "K = input_sequence @ random_weights_key\n",
    "V = input_sequence @ random_weights_value\n",
    "\n",
    "print(\"Size of Q:\", Q.size())\n",
    "print(\"Size of K:\", K.size())\n",
    "print(\"Size of V:\", V.size())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
