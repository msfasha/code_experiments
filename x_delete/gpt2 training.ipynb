{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import DataCollatorForLanguageModeling, GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments\n",
    "import pandas as pd\n",
    "\n",
    "# Load the GPT2 tokenizer and add a padding token\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set padding token to eos token\n",
    "\n",
    "# Load the dataset from parquet file\n",
    "data = pd.read_parquet(\"C:\\\\Users\\\\USER\\\\Downloads\\\\0005.parquet\")\n",
    "\n",
    "# Assuming the text data is in a column named 'text'\n",
    "texts = data['text'].tolist()\n",
    "\n",
    "# Select a portion of the dataset (1/10 of 250MB equivalent)\n",
    "portion_size = int(len(texts) / 10)\n",
    "small_texts = texts[:portion_size]\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(texts):\n",
    "    return tokenizer(texts, padding='max_length', truncation=True, max_length=1024)\n",
    "\n",
    "tokenized_texts = tokenize_function(small_texts)\n",
    "\n",
    "# Ensure all token IDs are within the valid range\n",
    "def ensure_valid_token_ids(tokenized_data, tokenizer):\n",
    "    input_ids = tokenized_data['input_ids']\n",
    "    attention_mask = tokenized_data['attention_mask']\n",
    "    vocab_size = len(tokenizer)\n",
    "    \n",
    "    new_input_ids = []\n",
    "    for ids in input_ids:\n",
    "        new_ids = [\n",
    "            tokenizer.convert_tokens_to_ids(tokenizer.convert_ids_to_tokens(id)) \n",
    "            if tokenizer.convert_tokens_to_ids(tokenizer.convert_ids_to_tokens(id)) is not None and tokenizer.convert_tokens_to_ids(tokenizer.convert_ids_to_tokens(id)) < vocab_size \n",
    "            else tokenizer.convert_tokens_to_ids(tokenizer.pad_token) \n",
    "            for id in ids\n",
    "        ]\n",
    "        new_input_ids.append(new_ids)\n",
    "    \n",
    "    tokenized_data['input_ids'] = new_input_ids\n",
    "    return tokenized_data\n",
    "\n",
    "tokenized_texts = ensure_valid_token_ids(tokenized_texts, tokenizer)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tokenized_data):\n",
    "        self.input_ids = tokenized_data['input_ids']\n",
    "        self.attention_mask = tokenized_data['attention_mask']\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': torch.tensor(self.input_ids[idx]),\n",
    "            'attention_mask': torch.tensor(self.attention_mask[idx])\n",
    "        }\n",
    "\n",
    "# Create an instance of the custom dataset\n",
    "train_dataset = CustomDataset(tokenized_texts)\n",
    "\n",
    "# Create a data collator for language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # We are not using masked language modeling for GPT-2\n",
    ")\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=500,  # Log every 500 steps\n",
    "    report_to=\"all\"  # Report to all available loggers (TensorBoard, console, etc.)\n",
    ")\n",
    "\n",
    "# Initialize the model\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model.resize_token_embeddings(len(tokenizer))  # Resize token embeddings to account for new tokens\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An optimized version of the above algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\envs\\myenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\USER\\anaconda3\\envs\\myenv\\lib\\site-packages\\transformers\\optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmsfasha\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\USER\\mywork\\mycode\\testing\\wandb\\run-20240619_203416-9lfrtpoz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/msfasha/huggingface/runs/9lfrtpoz' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/msfasha/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/msfasha/huggingface' target=\"_blank\">https://wandb.ai/msfasha/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/msfasha/huggingface/runs/9lfrtpoz' target=\"_blank\">https://wandb.ai/msfasha/huggingface/runs/9lfrtpoz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/19 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DataCollatorForLanguageModeling, GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments, AdamW, get_linear_schedule_with_warmup\n",
    "import pandas as pd\n",
    "\n",
    "# Enable CUDA launch blocking\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "# Load the GPT2 tokenizer and add a padding token\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set padding token to eos token\n",
    "\n",
    "# Load the dataset from parquet file\n",
    "data = pd.read_parquet(\"C:\\\\Users\\\\USER\\\\Downloads\\\\0005.parquet\")\n",
    "\n",
    "# Assuming the text data is in a column named 'text'\n",
    "texts = data['text'].tolist()\n",
    "\n",
    "# Select a portion of the dataset (1/10 of 250MB equivalent)\n",
    "portion_size = int(len(texts) / 100)\n",
    "small_texts = texts[:portion_size]\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(texts):\n",
    "    return tokenizer(texts, padding='max_length', truncation=True, max_length=1024)\n",
    "\n",
    "tokenized_texts = tokenize_function(small_texts)\n",
    "\n",
    "# Ensure all token IDs are within the valid range\n",
    "def ensure_valid_token_ids(tokenized_data, tokenizer):\n",
    "    input_ids = tokenized_data['input_ids']\n",
    "    attention_mask = tokenized_data['attention_mask']\n",
    "    vocab_size = len(tokenizer)\n",
    "    \n",
    "    new_input_ids = []\n",
    "    for ids in input_ids:\n",
    "        new_ids = [\n",
    "            tokenizer.convert_tokens_to_ids(tokenizer.convert_ids_to_tokens(id)) \n",
    "            if tokenizer.convert_tokens_to_ids(tokenizer.convert_ids_to_tokens(id)) is not None and tokenizer.convert_tokens_to_ids(tokenizer.convert_ids_to_tokens(id)) < vocab_size \n",
    "            else tokenizer.convert_tokens_to_ids(tokenizer.pad_token) \n",
    "            for id in ids\n",
    "        ]\n",
    "        new_input_ids.append(new_ids)\n",
    "    \n",
    "    tokenized_data['input_ids'] = new_input_ids\n",
    "    return tokenized_data\n",
    "\n",
    "tokenized_texts = ensure_valid_token_ids(tokenized_texts, tokenizer)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tokenized_data):\n",
    "        self.input_ids = tokenized_data['input_ids']\n",
    "        self.attention_mask = tokenized_data['attention_mask']\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': torch.tensor(self.input_ids[idx]),\n",
    "            'attention_mask': torch.tensor(self.attention_mask[idx])\n",
    "        }\n",
    "\n",
    "# Create an instance of the custom dataset\n",
    "train_dataset = CustomDataset(tokenized_texts)\n",
    "\n",
    "# Create a data collator for language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # We are not using masked language modeling for GPT-2\n",
    ")\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=16,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=500,  # Log every 500 steps\n",
    "    report_to=\"all\",  # Report to all available loggers (TensorBoard, console, etc.)\n",
    "    no_cuda=False,\n",
    "    fp16=True,\n",
    "    gradient_accumulation_steps=4,\n",
    "    dataloader_num_workers=4,\n",
    "    dataloader_pin_memory=True\n",
    ")\n",
    "\n",
    "# Initialize the model\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model.resize_token_embeddings(len(tokenizer))  # Resize token embeddings to account for new tokens\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "num_training_steps = len(train_dataset) // training_args.per_device_train_batch_size * training_args.num_train_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    optimizers=(optimizer, scheduler)\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the Saved Model\n",
    "To load the saved model and tokenizer for inference or further training, you can use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load the trained model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained(\"./results\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"./results\")\n",
    "\n",
    "# Use the model and tokenizer for inference or further training\n",
    "inputs = tokenizer(\"Hello, how are you?\", return_tensors=\"pt\")\n",
    "outputs = model.generate(inputs.input_ids)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary\n",
    "Checkpoints are saved in the ./results directory at intervals defined by save_steps.\n",
    "Logs are saved in the ./logs directory.\n",
    "The final model and tokenizer are saved in the ./results directory at the end of the training.\n",
    "This setup ensures that you have access to intermediate checkpoints and the final trained model, which can be used for evaluation, inference, or further fine-tuning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
