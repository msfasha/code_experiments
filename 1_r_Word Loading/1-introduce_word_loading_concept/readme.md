Languages differ not merely in vocabulary size but in their capacity to represent multidimensional human experience within single lexical itemsâ€”what we term "semantic loading." This paper introduces a computational framework for quantifying loading: the density and specificity with which individual words encode meaning across multiple semantic dimensions. We propose a loading metric based on word embedding analysis and demonstrate its application across three languages using parallel corpora. Our findings suggest that semantic loading is measurable, language-variable, and potentially correlates with documented linguistic features. This framework enables comparative analysis of representational capacity and opens avenues for investigating language-specific communicative strengths. We discuss implications for linguistic typology and applications in NLP.

This work and this paper investigates this concept of semantic loading, proposing a formal framework to quantify it and demonstrating its measurability across languages. We define semantic loading precisely, propose a computational metric based on word embeddings, test its applicability using parallel corpora, and discuss implications for linguistic theory and comparative linguistics.
