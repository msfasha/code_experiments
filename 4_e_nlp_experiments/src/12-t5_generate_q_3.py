# Code generated by ChatGPT to fine tune a T5 model for questions answering

import torch
from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments
from torch.utils.data import Dataset, DataLoader

# Define a custom dataset class for your data
class CustomDataset(Dataset):
    def __init__(self, tokenizer, data_file, max_length=128):
        self.data = []
        self.tokenizer = tokenizer
        self.max_length = max_length

        # Load your dataset from a file
        with open(data_file, 'r', encoding='utf-8') as file:
            for line in file:
                line = line.strip().split('\t')
                context = line[0]
                question = line[1]
                answer = line[2]

                # Format the data as T5 input
                input_text = f"context: {context} question: {question}"
                target_text = f"{answer}"

                # Tokenize and encode the data
                inputs = tokenizer(
                    input_text,
                    max_length=self.max_length,
                    padding="max_length",
                    truncation=True,
                    return_tensors="pt"
                )
                targets = tokenizer(
                    target_text,
                    max_length=self.max_length,
                    padding="max_length",
                    truncation=True,
                    return_tensors="pt"
                )

                self.data.append({
                    'input_ids': inputs['input_ids'].squeeze(),
                    'attention_mask': inputs['attention_mask'].squeeze(),
                    'decoder_input_ids': targets['input_ids'].squeeze(),
                    'decoder_attention_mask': targets['attention_mask'].squeeze()
                })

    def __len__(self):
        return len(self.data)

    def __getitem__(self, index):
        return self.data[index]

# Initialize the T5 model and tokenizer
model_name = "t5-small"  # You can choose a different model size if needed
tokenizer = T5Tokenizer.from_pretrained(model_name)
model = T5ForConditionalGeneration.from_pretrained(model_name)

# Define your training data file
data_file = "your_dataset.txt"

# Create a custom dataset
dataset = CustomDataset(tokenizer, data_file)

# Define training arguments
training_args = TrainingArguments(
    output_dir="./t5-fine-tuned",
    num_train_epochs=3,  # Adjust as needed
    per_device_train_batch_size=4,  # Adjust as needed
    evaluation_strategy="steps",
    save_steps=1000,
    eval_steps=1000,
    logging_steps=100,
    learning_rate=1e-4,  # Adjust as needed
    save_total_limit=2,
    remove_unused_columns=False,
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    greater_is_better=False,
)

# Initialize a Trainer instance for fine-tuning
trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=None,  # You can use the default data collator for text-to-text tasks
    train_dataset=dataset,
)

# Fine-tune the model
trainer.train()

# Save the fine-tuned model
model.save_pretrained("./t5-fine-tuned")
tokenizer.save_pretrained("./t5-fine-tuned")
