# The objective of this code is to initially test the Rouge and the BLEU metrics
# Rouge focuses on Recall while BLEU focuses in Precision
# The code is generated by ChatGPT, after a discussion about some of the used benchmarks to test LLM performace
# Here is the initial question
# give me brief information about the following benchmarks:
# BoolQ	PIQA	HellaSwag	WinoGrande	ARC-e	ARC-c	OBQA	Avg
# The above measures were mentioned in GPT4All page https://gpt4all.io/index.html

#################################################################################
# TEST ROUGE
#################################################################################
# Results for ROUGE
# ROUGE-1: Score(precision=0.3333333333333333, recall=0.3333333333333333, fmeasure=0.3333333333333333)
# ROUGE-2: Score(precision=0.125, recall=0.125, fmeasure=0.125)
# ROUGE-L: Score(precision=0.3333333333333333, recall=0.3333333333333333, fmeasure=0.3333333333333333)

from rouge_score import rouge_scorer

# Example generated and reference texts
generated_text = "The quick brown fox jumps over the lazy dog"
reference_text = "A fast, brown fox leaps over a sleeping canine"

# Initialize the ROUGE scorer
scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'])

# Calculate ROUGE scores
scores = scorer.score(generated_text, reference_text)

# Access individual ROUGE scores
rouge1_score = scores['rouge1']
rouge2_score = scores['rouge2']
rougeL_score = scores['rougeL']

# Print the ROUGE scores
print("ROUGE-1:", rouge1_score)
print("ROUGE-2:", rouge2_score)
print("ROUGE-L:", rougeL_score)

#################################################################################
# TEST BLEU
#################################################################################
# BLEU Score: 6.030654703641957e-155

import nltk
nltk.download('punkt')

# Example generated and reference texts
generated_text = "The quick brown fox jumps over the lazy dog"
reference_text = "A fast, brown fox leaps over a sleeping canine"

# Tokenize the texts (split into words)
generated_tokens = nltk.word_tokenize(generated_text.lower())
reference_tokens = nltk.word_tokenize(reference_text.lower())

# Calculate BLEU score
bleu_score = nltk.translate.bleu_score.sentence_bleu([reference_tokens], generated_tokens)

# Print the BLEU score
print("BLEU Score:", bleu_score)

